---
title: "Sequencing 16S rRNA gene fragments using the PacBio SMRT DNA sequencing system"
author: "Patrick D. Schloss"
date: "July 21, 2014"
output:
	word_document:
		keep_md: true
---

**Running title:** 16S rRNA genes sequencing with PacBio

**Authors:** Patrick D. Schloss^1#^, Sarah L. Westcott^1^, Matthew L. Jenior^1^,
and Sarah K. Highlander^2^

* Correspondence:	pschloss@umich.edu
734.647.5801
Department of Microbiology and Immunology
University of Michigan
5618 Medical Sciences II
1500 W. Medical Center Dr.
Ann Arbor, MI  48109

1	Department of Microbiology and Immunology, 1500 W. Medical Center, University
of Michigan, Ann Arbor, MI 48109
2	J Craig Venter Institute, 4120 Torrey Pines Road, La Jolla, CA 92307


## Abstract



### Keywords
Microbial ecology, bioinformatics, sequencing error



## Introduction
The advent of so-called next generations sequencing technologies have introduced
considerable advances in the field of microbial ecology. Clone-based Sanger
sequencing of the 16S rRNA gene has largely been replaced by various platforms
produced by 454/Roche [ref], Illumina [ref], and IonTorrent [ref]. It was once
common to sequence fewer than 100 16S rRNA gene sequence from several samples
using the Sanger approach. Now it is common to generate thousands of sequences
from each of several hundred samples [ref]. This advance in throughput has come
at the cost of read length. Sanger sequencing regularly generated 800 nt per
read and because the DNA was cloned, it was possible to obtain multiple reads
per fragment to yield a full-length sequence [ref]. At a cost of approximately
$8 (US) per sequencing read, most researchers have effectively decided that
full-length sequences are not worth the increased cost relative to the cost of
the next-generation approaches.

Each of these sequencing platforms has been primarily created to perform genome
sequencing. When sequencing a genome, it is assumed that the same base of DNA
will be sequenced multiple times and the consensus of multiple sequence reads is
used to generate contigs. Thus, although an individual base call may have a high
error rate, the consensus sequence will have a low error rate. To sequence the
16S rRNA gene researchers use conserved primers to amplify a sub-region from
within the gene that is isolated from many organisms. Because of the fragments
are not cloned, it is not possible to obtain high sequence coverage from the
same DNA molecule using these platforms. Thus, to reduce sequencing error rates
it has become imperative to develop sequence curation and denoising algorithms.
There has been a tradeoff between read length, number of reads per sample, and
the error rate. For instance, we recently demonstrated that using the Illumina
MiSeq and the 454 Roche Titanium platforms the raw error rate varies between 1
and 2% [refs]. Yet, it was possible to obtain error rates below 0.02% by
adopting various denoising algorithms. However, the resulting fragments were
only 250 bp. In the case of 454 Roche Titanium, extending the length of the
fragment introduces length-based errors and in the case of the Illumina MiSeq,
increasing the length of the fragment reduces the overlap between the read pairs
reducing the ability of each read to mutually reduce the sequencing error.
Although both of these platforms enjoy widespread use in the field, the MiSeq
platform is emerging as the leader because of the ability to sequence 15-20
million fragments that can be distributed across hundreds of samples for less
than $5000 (US).

As these sequencing platforms have grown in popularity, there has been a decline
in the number of full-length 16S rRNA genes being deposited into GenBank that
could serve as references. This is particularly frustrating since the
technologies have significantly reduced our limit of detection only to identify
novel populations for which we lack full-length reference sequences. A related
problem is the perceived limitation that the short reads generated by the 454
Roche and Illumina platforms cannot be reliably classified to the genus or
species level [ref]. Previous investigators have utilized simulations to
demonstrate that increased read lengths usually increase the accuracy and
sensitivity of classification against reference databases [ref]. There is
clearly a need to develop sequencing technologies that will allow researchers to
generate high quality full-length 16S rRNA gene sequences in a high throughput
manner.

New advances in single molecule sequencing technologies, such as the platform
produced by Pacific Biosciences (PacBio), offer the opportunity to once again
obtain full-length sequence reads with a high depth of coverage from a large
number of samples. To this point, the PacBio Single Molecule, Real-Time (SMRT)
DNA Sequencing System has received limited application in the microbial ecology
research domain [ref]. The SMRT system ligates hairpin adapters (i.e. SMRTbells)
to the ends of double-stranded DNA. Although the DNA molecule is linear, it is
effectively circularized allowing the sequencing polymerase to process around
the molecule multiple times. According to Pacific Biosciences the platform is
able to generate median read lengths longer than 8 kb with the P4-C2 chemistry;
however, the single pass error rate of is 12-15%
[http://files.pacb.com/pdf/PacBio_RS_II_Brochure.pdf]. Given the circular nature
of the DNA fragment, the full read length can be used to cover the DNA fragment
multiple times resulting in a reduced error rate. Therefore, one should be able
to obtain multiple coverage of the full 16S rRNA gene at a reduced error rate.

Despite the opportunity to potentially generate high-quality full-length
sequences, the Pacific Biosciences platform has not been widely adopted for
sequencing 16S rRNA genes. Previous studies utilizing the technology have
removed reads with mismatched primers and barcodes, ambiguous base calls, and
low quality scores; however, the error rates associated with these criteria were
not reported [ref]. Others have utilized the platform without describing the
bioinformatic pipeline that was utilized [ref]. In the current study, we sought
to assess the quality of data generated by the Pacific Biosciences sequencer and
whether it could fill the need for generating high-quality, full-length sequence
data. We hypothesized that by modulating the 16S rRNA gene fragment length we
could alter the read depth and obtain reads longer than are currently available
by the 454 Roche and Illumina platforms but with the same quality. To test this
hypothesis, we developed a sequence curation pipeline that was optimized by
reducing the sequencing error rate of a mock bacterial community with known
composition. The resulting pipeline was then applied to 16S rRNA gene fragments
that were isolated from soil and human and mouse feces.


## Materials and Methods
### Community DNA
We utilized genomic DNA isolated from four communities. These same DNA extracts
were previously used to develop an Illumina MiSeq-based sequencing strategy
[ref].  Briefly, we used a “Mock Community” composed of genomic DNA from 21
bacterial isolates: *Acinetobacter baumannii* ATCC 17978, *Actinomyces
odontolyticus* ATCC 17982, *Bacillus cereus* ATCC 10987, *Bacteroides vulgatus*
ATCC 8482, *Clostridium beijerinckii* ATCC 51743, *Deinococcus radiodurans* ATCC
13939, *Enterococcus faecalis* ATCC 47077, *Escherichia coli* ATCC 70096,
*Helicobacter pylori* ATCC 700392, *Lactobacillus gasseri* ATCC 33323, *Listeria
monocytogenes* ATCC BAA-679, *Neisseria meningitidis* ATCC BAA-335,
*Porphyromonas gingivalis* ATCC 33277, *Propionibacterium acnes* DSM 16379,
*Pseudomonas aeruginosa* ATCC 47085, *Rhodobacter sphaeroides* ATCC 17023,
*Staphylococcus aureus* ATCC BAA-1718, *Staphylococcus epidermidis* ATCC 12228,
*Streptococcus agalactiae* ATCC BAA-611, *Streptococcus mutans* ATCC 700610,
*Streptococcus pneumoniae* ATCC BAA-334. The mock community DNA is available
through BEI resources (v3.1, HM-278D). Genomic DNAs from the three other
communities were obtained using the MO BIO PowerSoil DNA extraction kit. The
human and mouse fecal samples were obtained using protocols that were reviewed
and approved by the University Committee on Use and Care of Animals and the
Institutional Review Board at the University of Michigan.

### Library generation and sequencing
The DNAs were each amplified in triplicate using barcoded primers targeting the
V4, V1-V3, V3-V5, V1-V5, V1-V6, and V1-V9 (Table 1). The primers were
synthesized so that the 5’ end of the forward and reverse primers were each
tagged with a 5-nt barcode sequence to allow multiplexing of samples within a
single sequencing run. Methods describing PCR, amplicon cleanup, and pooling
were described previously [ref]. The SMRTbell adapters were ligated onto the PCR
products and sequenced at the University of Michigan DNA Sequencing Core using
the P4-C2 chemistry on a PacBio RS II SMRT DNA Sequencing System.

### Data analysis
All sequencing data were curated using mothur (v 1.34) and analyzed using the R
programming language [refs]. Several specific features were incorporated to
facilitate the analysis of PacBio sequence data. First, because non-ambiguous
base calls are assigned to Phred quality scores of zero, the consensus fastq
files were parsed so that scores of zero were interpreted as corresponding to an
ambiguous base call (i.e. N) in the fastq.info command using the pacbio=T
option. Second, because the consensus sequence can be generated in the correct
and reverse complement orientations, a checkorient option was added to the
trim.seqs command in order to identify the proper orientation. These features
were incorporated into mothur v.1.30. We identified chimeras and assessed error
rates in the mock community as described previously using the seq.error command
in mothur [refs]. Detailed methods are available as a public online repository
(http://github.com/SchlossLab/PacBio_16S) and the original data and all
derivatives can be obtained at FigShare. A standard operating procedure for
analyzing PacBio data is available on the mothur wiki
(http://www.mothur.org/wiki/PacBio_SOP).

```{r setup, echo=TRUE, eval=TRUE, tidy=TRUE}
regions <- c("v4", "v13", "v35", "v15", "v16", "v19")
clrs <- rainbow(length(regions))
pretty.region <- c("v13"="V1-V3", "v15"="V1-V5", "v16"="V1-V6", "v19"="V1-V9", "v35"="V3-V5", "v4"="V4")
names(clrs) <- regions

opts_chunk$set("fig.path"="figure/")
opts_chunk$set("fig.align"="center")
opts_chunk$set("dev" = c("png", "pdf"))

opts_chunk$set("tidy" = TRUE)

opts_chunk$set("echo" = TRUE)
opts_chunk$set("eval" = TRUE)
opts_chunk$set("warning" = FALSE)
```

## Results and Discussion

### The PacBio error profile

```{r error_profile}
sub.matrix <- read.table(file=paste("pipeline_dev/", regions[1], "/", regions[1], ".mock.filter.error.matrix", sep=""), header=T, row.names=1)[-7,]
for(i in 2:length(regions)){
	sub.matrix <- sub.matrix + read.table(file=paste("pipeline_dev/", regions[i], "/", regions[i], ".mock.filter.error.matrix", sep=""), header=T, row.names=1)[-7,]
}

matches <- c(sub.matrix[1,1], sub.matrix[2,2], sub.matrix[3,3], sub.matrix[4,4])
total <- sum(sub.matrix)
errors <- sub.matrix
diag(errors[1:4,1:4]) <- 0
init.error.rate <- 100 * sum(errors) / total				#1.925717

subst.bias <- apply(errors[1:4,1:4], 2, sum)/apply(sub.matrix[1:4,1:4], 2, sum)
subst.bias <- 100 * subst.bias/sum(subst.bias)
#       rA        rT        rG        rC 
#0.2247412 0.2748158 0.2392554 0.2611875 
#these are more or less equal to each other

insert.bias <- (sub.matrix[,"rGap"]/apply(sub.matrix, 1, sum))[1:4]
insert.bias <- 100 * insert.bias/sum(insert.bias)
#       qA        qT        qG        qC
#0.2364246 0.2432987 0.2425315 0.2777452
#these are more or less equal to each other

delete.bias <- (sub.matrix["qGap",]/apply(sub.matrix, 2, sum))[1:4]
delete.bias <- 100 * delete.bias/sum(delete.bias)
#            rA        rT       rG        rC
#qGap 0.1167635 0.0978054 0.442078 0.3433531
#interestingly the G's and C's are more likely to be deleted than the As or Ts

substitutions <- 100*sum(errors[1:4,1:4])/sum(errors)	#0.3656617
insertions <- 100*sum(errors[,5])/sum(errors)			#0.450299
deletions <- 100*sum(errors[5,])/sum(errors)			#0.167206
ambiguous <- 100*sum(errors[6,])/sum(errors)			#0.02157014


# Need to connect error types with the quality scores
error.quality <- read.table(file=paste("pipeline_dev/", regions[1], "/", regions[1], ".mock.filter.error.quality", sep=""), header=T, row.names=1)[-7,]
for(i in 2:length(regions)){
	error.quality <- error.quality + read.table(file=paste("pipeline_dev/", regions[i], "/", regions[i], ".mock.filter.error.quality", sep=""), header=T, row.names=1)[-7,]
}
error.quality["72",] <- error.quality["72",] + error.quality["80",]
error.quality <- error.quality[1:72,]

#get quantiles
matches <- rep(as.numeric(rownames(error.quality)), error.quality$matches)
m.quant <- quantile(matches, prob=c(0.025, 0.25, 0.5, 0.75, 0.975))

subs <- rep(as.numeric(rownames(error.quality)), error.quality$substitutions)
s.quant <- quantile(subs, prob=c(0.025, 0.25, 0.5, 0.75, 0.975))

ins <- rep(as.numeric(rownames(error.quality)), error.quality$insertions)
i.quant <- quantile(ins, prob=c(0.025, 0.25, 0.5, 0.75, 0.975))

ambig <- rep(as.numeric(rownames(error.quality)), error.quality$ambiguous)
a.quant <- quantile(ambig, prob=c(0.025, 0.25, 0.5, 0.75, 0.975))
```

To build a sequence curation pipeline, we first needed to characterize the error
rate associated with sequencing the 16S rRNA gene. We observed a sequencing
error rate of `r format(init.error.rate, digits=3, nsmall=2)`% across the regions within
the gene that we tested. Insertions, deletions, substitutions, and ambiguous base calls
accounted for `r format(insertions, digits=3, nsmall=1)`,
`r format(deletions, digits=3, nsmall=1)`, `r format(substitutions, digits=3, nsmall=1)`,
and `r format(ambiguous, digits=2, nsmall=1)` of
the errors, respectively. The substitution errors were equally likely and all four bases
were equally likely to cause insertion errors. Interestingly, guanines
(`r format(delete.bias["rG"], digits=3, nsmall=1)`%) and cytosines
(`r format(delete.bias["rC"], digits=3, nsmall=1)`%) were more likely to be deleted
than adenines (`r format(delete.bias["rA"], digits=3, nsmall=1)`%) or thymidines
(`r format(delete.bias["rT"], digits=3, nsmall=1)`%). When we considered the Phred
quality score of each base call, we observed a median quality score of `r m.quant["50%"]`
for correct base calls and scores of `r s.quant["50%"]` and `r i.quant["50%"]` for
substitutions and insertions, respectively (Figure FigureErrorQualityScore_A). Although there
was a broad distribution of quality scores with each type of base call, the errors could
largely be distinguished from the correct base calls.


### A basic sequence curation procedure

```{r}
getInitError <- function(folder){
	error <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".mock.filter.error.summary", sep=""), header=T, row.names=1)
	error <- error[error$numparents==1,]
	return(sum(error$mismatches)/sum(error$total))
}

init.rates <- unlist(lapply(regions, getInitError))
names(init.rates) <- regions
#          v4         v13         v35         v15         v16         v19 
# 0.008257847 0.023997519 0.025057346 0.018050725 0.025606045 0.022916201 


getReasonsLost <- function(region){
	composite <- read.table(file=paste("pipeline_dev/", region, "/", region, ".composite", sep=""), header=T, row.names=1)
	reasons <- as.character(composite$reason)

	nseqs <- length(reasons)
	good <- sum(reasons=="g")	/ nseqs
	start.stop <- sum(grepl("s", reasons) | grepl("e", reasons)) / nseqs
	homop <- length(grep("h", reasons)) / nseqs
	ambig <- length(grep("n", reasons)) / nseqs
	return(c(good=good, start.stop=start.stop, homop=homop, ambig=ambig, ngood=nseqs))
}

reasons.lost <- matrix(unlist(lapply(regions, getReasonsLost)), ncol=5, byrow=T)
colnames(reasons.lost) <- c("good", "start.stop", "homop", "ambig", "nseqs")
rownames(reasons.lost) <- regions

nseqs <- reasons.lost[,"nseqs"]
n.good <- nseqs * reasons.lost[,1]
reasons.lost <- reasons.lost[,-5]

getBasicError <- function(folder){
	composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
	good <- composite[composite$reason == "g",]
	return(sum(good$mismatches)/sum(good$total))
}

basic.error <- unlist(lapply(regions, getBasicError))
names(basic.error) <- regions
#          v4         v13         v35         v15         v16         v19 
# 0.005832224 0.015089630 0.014237063 0.009483316 0.014764301 0.013538793 
composite.basic.error <- 100 * sum(basic.error*n.good)/(sum(n.good))
```

To establish a simple curation procedure, we culled any sequence that contained
an ambiguous base call, had a string of the same base repeated 9 or more times,
did not start and end at the expected alignment coordinates for that region
of the 16S rRNA gene, or that was chimeric. This reduced the experiment-wide
error rate from `r format(init.error.rate, digits=3, nsmall=2)` to `r format(composite.basic.error, digits=2, nsmall=2)`% across the six regions we considered. This basic
procedure resulted in the removal of between `r round(min(100*(1-reasons.lost[,"good"])), digits=1)` (`r pretty.region[names(which.min(100*(1-reasons.lost[,"good"])))]`) and `r round(max(100*(1-reasons.lost[,"good"])), digits=1)` (`r pretty.region[names(which.max(100*(1-reasons.lost[,"good"])))]`)% of the
reads and the percentage of reads removed increased with the length of the
fragment (Figure PipelineError). For each region, the number of reads removed
because of the presence of ambiguous base calls was similar to the number of
reads that were removed for not fully aligning to the correct region within the
16S rRNA gene (Table SupplementalReasonsLost). The latter class of errors was
generally due to sequence truncations that could not be explained.  


### Identifying correlates of increased sequencing error

```{r barcodePrimerAnalysis}
getFragment_BPErrorRates <- function(folder){
	composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
	composite <- composite[composite$reason == "g",]
	bc.primer <- composite$fbdiffs + composite$rbdiffs + composite$fpdiffs + composite$rpdiffs

	return(cbind(composite$error, bc.primer))
}

errors <- lapply(regions, getFragment_BPErrorRates)

errors.table <- matrix(errors[[1]], ncol=2, byrow=F)
err.bcprimer <- aggregate(100*errors.table[,1], by=list(errors.table[,2]), function(x){c(mean(x), length(x))})

getBCPrimerError <- function(folder){
	composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
	composite <- composite[composite$reason == "g",]
	mismatch <- composite$fbdiffs + composite$rbdiffs + composite$fpdiffs + composite$rpdiffs

	good0 <- mismatch == 0
	composite.good0 <- composite[good0,]
	
	good1 <- mismatch <= 1
	composite.good1 <- composite[good1,]
	
	return(c(mean(composite.good0[,"error"]), nrow(composite.good0)/nrow(composite),
	mean(composite.good1[,"error"]), nrow(composite.good1)/nrow(composite) ))
}

bcprimer.error <- matrix(unlist(lapply(regions, getBCPrimerError)), nrow=6, byrow=T)
rownames(bcprimer.error) <- regions
colnames(bcprimer.error) <- c("0.error", "0.frac", "1.error", "1.frac")

#         0.error    0.frac     1.error    1.frac
# v4  0.002616919 0.6312137 0.003711755 0.8932658
# v13 0.011102712 0.5731377 0.012493712 0.8573363
# v35 0.010838504 0.6398867 0.011901910 0.8844452
# v15 0.005375052 0.5023119 0.007029568 0.8246322
# v16 0.010374438 0.5399263 0.011874367 0.8519656
# v19 0.008543379 0.4998548 0.010086351 0.8117920
```


```{r coverageAnalysis}
errorAtCoverageCutoff <- function(folder){
	composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
	composite <- composite[composite$reason == "g",]

	total.seqs <- nrow(composite)

	deep <- composite$freq >= 10
	composite <- composite[deep,]

	covered.seqs <- nrow(composite)

	error <- sum(composite$mismatches)/sum(composite$total)
	frac.kept <- covered.seqs / total.seqs
	return(c(error=error, kept=frac.kept))
}

coverage.error <- unlist(lapply(regions, errorAtCoverageCutoff))
coverage.error <- matrix(coverage.error, ncol=2, byrow=T)
rownames(coverage.error) <- regions
colnames(coverage.error) <- c("10.error", "10.frac")
       
#        10.error   10.frac
# v4  0.002812434 0.7875164
# v13 0.010385450 0.6568849
# v35 0.010492018 0.6627146
# v15 0.006928824 0.6698613
# v16 0.010906203 0.4606880
# v19 0.009889442 0.5620099


coverage.reduction <- (1-coverage.error[,"error"]/basic.error)*100
#      v16      v35      v15      v19      v13       v4 
# 26.13126 26.30490 26.93670 26.95477 31.17492 51.77766 
```


```{r qualityScoreAnalysis}
getErrorRateFromAveQ <- function(folder, threshold=60){
	write(folder, "")
	composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
	composite <- composite[composite$reason == "g",]

	rates <- composite[composite$aveQ >= threshold,]
	
	remaining.error <- mean(rates[,"error"])
	fraction.kept <- nrow(rates)/nrow(composite)
	
	return(c(remaining.error, fraction.kept))
}

aveq.error <- matrix(unlist(lapply(regions, getErrorRateFromAveQ)), ncol=2, byrow=T)
rownames(aveq.error) <- regions
colnames(aveq.error) <- c("60.error", "60.frac")

#        60.error   60.frac
# v4  0.002923691 0.8667229
# v13 0.009899221 0.7571106
# v35 0.009427016 0.7676517
# v15 0.004816332 0.6967633
# v16 0.009527474 0.6560197
# v19 0.005936197 0.4978217

quality.reduction <- (1-aveq.error[,"error"]/basic.error)*100
```

```{r oligosCoverageAnalysis}
oligosCoverage <- function(folder){
	write(folder, "")
	composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
	composite <- composite[composite$reason == "g",]
	
	mismatch <- composite$fbdiffs + composite$rbdiffs + composite$fpdiffs + composite$rpdiffs
	good0 <- mismatch == 0 & composite$freq >= 10
	composite.good0 <- composite[good0,]
	
	good1 <- mismatch <= 1 & composite$freq >= 10
	composite.good1 <- composite[good1,]
	
	return(c(mean(composite.good0[,"error"]), nrow(composite.good0)/nrow(composite),
	mean(composite.good1[,"error"]), nrow(composite.good1)/nrow(composite) ))
}

oligosCoverage.error <- matrix(unlist(lapply(regions, oligosCoverage)), ncol=4, byrow=T)
rownames(oligosCoverage.error) <- regions
colnames(oligosCoverage.error) <- c("0.10.error", "0.10.frac", "1.10.error", "1.10.frac")

#      0.10.error 0.10.frac  1.10.error 1.10.frac
# v4  0.001565052 0.5476458 0.002068184 0.7389796
# v13 0.009371691 0.4302483 0.009899256 0.6108352
# v35 0.009096211 0.4857547 0.009503092 0.6257300
# v15 0.004050098 0.3762085 0.005253236 0.5834384
# v16 0.007945951 0.2929975 0.008956478 0.4256757
# v19 0.006706336 0.3142608 0.007903900 0.4856230
```

```{r oligosQScoreAnalysis}
oligosQScore <- function(folder){
	write(folder, "")
	composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
	composite <- composite[composite$reason == "g",]

	mismatch <- composite$fbdiffs + composite$rbdiffs + composite$fpdiffs + composite$rpdiffs

	good <- mismatch == 0 & composite$aveQ >= 60
	composite.good0 <- composite[good,]
	
	good1 <- mismatch <= 1 & composite$aveQ >= 60
	composite.good1 <- composite[good1,]
	
	return(c(mean(composite.good0[,"error"]), nrow(composite.good0)/nrow(composite),
	mean(composite.good1[,"error"]), nrow(composite.good1)/nrow(composite) ))
}

oligosQScore.error <- matrix(unlist(lapply(regions, oligosQScore)), ncol=4, byrow=T)
rownames(oligosQScore.error) <- regions
colnames(oligosQScore.error) <- c("0.60.error", "0.60.frac", "1.60.error", "1.60.frac")

#      0.60.error 0.60.frac  1.60.error 1.60.frac
# v4  0.001715135 0.5916338 0.002215499 0.8096980
# v13 0.009153895 0.4954853 0.009700915 0.7024831
# v35 0.008365403 0.5501681 0.008690839 0.7207574
# v15 0.003217812 0.4057167 0.003993206 0.6213535
# v16 0.007589961 0.4072482 0.008358922 0.6038084
# v19 0.004870705 0.3162939 0.005294429 0.4586117
```

```{r coverageQScoreAnalysis}
coverageQScore <- function(folder){
	write(folder, "")
		composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
		composite <- composite[composite$reason == "g",]
	
	good <- composite$freq >= 10 & composite$aveQ>= 60
	composite.good <- composite[good,]
	
	return(c(mean(composite.good[,"error"]), nrow(composite.good)/nrow(composite) ))	
}

coverageQScore.error <- matrix(unlist(lapply(regions, coverageQScore)), ncol=2, byrow=T)
rownames(coverageQScore.error) <- regions
colnames(coverageQScore.error) <- c("10.60.error", "10.60.frac")

#     10.60.error 10.60.frac
# v4  0.002636544  0.7850778
# v13 0.010013647  0.6514673
# v35 0.009707464  0.6549283
# v15 0.004775956  0.5793190
# v16 0.010487021  0.4588452
# v19 0.005815805  0.3860006
```

```{r allFiltersAnalysis}
allFilters <- function(folder){
write(folder, "")
		composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
		composite <- composite[composite$reason == "g",]
	mismatch <- composite$fbdiffs + composite$rbdiffs + composite$fpdiffs + composite$rpdiffs

good0 <- mismatch == 0 & composite$aveQ>= 60 & composite$freq >= 10
composite.good0 <- composite[good0,]

good1 <- mismatch <= 1 & composite$aveQ >= 60 & composite$freq >= 10
composite.good1 <- composite[good1,]

return(c(mean(composite.good0[,"error"]), nrow(composite.good0)/nrow(composite),
mean(composite.good1[,"error"]), nrow(composite.good1)/nrow(composite) ))
}

allFilters.error <- matrix(unlist(lapply(regions, allFilters)), ncol=4, byrow=T)
rownames(allFilters.error) <- regions
colnames(allFilters.error) <- c("0.10.60.error", "0.10.60.frac", "1.10.60.error", "1.10.60.frac")

#     0.10.60.error 0.10.60.frac 1.10.60.error 1.10.60.frac
# v4    0.001515500    0.5470831   0.001993325    0.7376665
# v13   0.009299146    0.4286682   0.009827404    0.6083521
# v35   0.008613200    0.4813307   0.008979207    0.6190055
# v15   0.003078476    0.3450189   0.003905562    0.5210593
# v16   0.007738039    0.2923833   0.008814923    0.4250614
# v19   0.004797594    0.2460064   0.005266467    0.3578275
```


In contrast to the 454 Roche and Illumina-based platforms where the sequencing
quality decays with length, the consensus sequencing approach employed by the
PacBio sequencer is thought to generate a uniform distribution of errors. This
makes it impossible to simply trim sequences to high quality regions. Therefore,
we sought to identify characteristics within sequences that would allow us to
identify and remove those sequences with errors using three different
approaches. First, we hypothesized that errors in the barcode and primer would
be correlated with the error rate for the entire sequence. We observed a strong
relationship between the number of mismatches to the barcodes and primers and the
error rate of the rest of the sequence fragment (Figure FigureErrorQualityScore_B). Although 
allowing no mismatches to the barcodes and primers yielded the lowest error rate,
that stringent criterion removed a large fraction of the reads from the dataset and 
allowing at most one mismatch marginally increased the error rate while preserving 
more sequences in the dataset (Figure PipelineError). Second, we hypothesized
that increased sequencing coverage should yield lower error rates. We found that
once we had obtained 10-fold coverage of the fragments, the error rate did not
change appreciably (Figure FigureErrorQualityScore_C). When we compared the error
rates of reads with at least 10-fold coverage to those with less coverage, we
reduced the error rate by `r paste(round(sort(coverage.reduction)[c(1,5)], digits=1), collapse=" to ")`% for each region except the V4 region for which
the error rate was reduced by `r round(coverage.reduction["v4"], digits=1)`%.
Third, based on the earlier analysis associating errors with quality scores, we used 
two quality score-based approaches for identifying reads with errors. We calculated 
the minimum average quality score across all 50-nt window within each sequence and we 
also calculated the average quality score across each sequence. We then associated 
both methods of calculating the average quality score with the error rate of the 
reads and the fraction of sequences that would be retained if each threshold were 
selected. Using the sliding window approach we did not observe any clear break points 
indicating that one quality score would be better than another. In contrast, using 
the whole sequence quality score average we observed a decrease in the error rate and 
the fraction of sequences retained when the threshold was increased above 60 (Figure 
QualityScoreAverage). When we used this threshold, we were able to reduce the error 
rate by `r paste(round(range(quality.reduction), digits=1), collapse=" to ")`% 
(Figure ErrorSequenceCoverage). We noted that the fraction of reads retained 
decreased as the length of the fragment increased with retention of
`r round(100*aveq.error["v4", "kept"],digits=1)`% of the V4 reads and
`r round(100*aveq.error["v19", "kept"],digits=1)`% of the V1-V9 reads (Figure 
ErrorSequenceCoverage). Next, we asked whether which combinations of culling 
reads with mismatches to the expected barcodes and primers, less than 10-fold 
sequencing coverage, and an average quality score less than 60 made the most 
meaningful reductions in the error rate while preserving the most reads. Finally, we
implemented the basic curation pipeline along with permutations of these
three critera. We observed similar error rates when we required 1 or fewer
mismatches to the barcodes and primers and an average quality score above 60 as
when we also required a minimum 10-fold coverage. Culling sequences that had more than
one mismatch to the barcodes and primers and those with an average quality score less
than 60 reduced the error rate to between `r paste(round(100*range(oligosQScore.error[,"1.60.error"]), digits=2), collapse=" and ")`. This procedure resulted in the removal of `r paste(rev(round(100*(1-range(oligosQScore.error[,"1.60.frac"])), digits=0)), collapse=" and ")`% of the reads (Figure PipelineError).


### Pre-clustering sequences to further reduce sequencing noise

```{r}
summarizeError <- function(folder, method){
  file1 <- paste(folder, "/", folder, ".mock1.", method, ".error.summary", sep="")
  file2 <- paste(folder, "/", folder, ".mock2.", method, ".error.summary", sep="")
  file3 <- paste(folder, "/", folder, ".mock3.", method, ".error.summary", sep="")

  summary <- rbind(read.table(file=file1, header=T, row.names=1), read.table(file=file2, header=T, row.names=1), read.table(file=file3, header=T, row.names=1))
  nochim <- summary[summary$numparents==1,]
  error <- sum(nochim$weight * nochim$mismatches) / sum(nochim$weight * nochim$total)
  return(error)
}

unique <- unlist(lapply(regions, summarizeError, "unique"))
precluster <- unlist(lapply(regions, summarizeError, "precluster"))
error <- cbind(unique, precluster)
rownames(error) <- regions

#         unique  precluster
#v13 0.008320245 0.007029193
#v15 0.003644766 0.001727743
#v16 0.006742346 0.005426787
#v19 0.004917835 0.003397170
#v35 0.009064385 0.007661003
#v4  0.001746057 0.000943217

```

Previously, we implemented a pre-clustering algorithm where sequences are sorted
by their abundance in decreasing order and rare sequences are clustered with a
more abundant sequence if the rare sequences have fewer mismatches than a
defined threshold when compared to the more abundant sequence. The recommended
threshold was a 1-nt difference per 100-bp of sequence data. For example, the
threshold for 250 bp fragment from the V4 region would be 2 nt or 14 for the
1458 bp V1-V9 fragments. This approach removes residual PCR and sequencing
errors while not overwhelming the resolution needed to identify OTUs that are
based on a 3% distance threshold. Needless to say, this approach would limit
one’s ability to detect 3 nt differences between V1-V9 sequences. When we apply
this approach to our PacBio data, we observed a reduction in the error rate
between 15 (V1-V3 and 	V3-V5) and 53% (V1-V5). The final error rates varied
between 0.09 (V4) and 0.77% (V3-V5); the full-length, V1-V9, fragments had an
error rate of 0.34% (Figure ErrorSequenceCoverage). These error rates are 4-5
times higher than what we have previously observed using the Roche 454 and
Illumina MiSeq platforms [refs].



### Effects of error rates on OTU assignments


The sequencing error rate is known to affect the number of OTUs that are
observed. For each region, we calculated the number of OTUs we would have
observed in the mock community if there were no PCR or sequencing errors or
chimeras, the number of OTUs if there were no chimeras, and the number of OTUs
observed using the pipeline described above and UCHIME to identify and cull
chimeras (Table OTU). We also calculated the number of OTUs in the soil, mouse,
and human samples using the same pipeline with chimera detection and removal.
Because we wanted to compare all datasets using the same number of reads and
because a large number of reads from the V1-V9 dataset were removed for quality
problems, we rarefied all datasets to 354 sequences per dataset. Among the mock
community samples the number of OTUs increased with error rate. Although the
variation in the number of OTUs is affected by the error rate, it is also
affected by the specificity of the PCR primers and the sequence variation within
the region being amplified.

```{r}
counts <- matrix(rep(0, length(regions)*12), nrow=length(regions))
rownames(counts) <- regions
colnames(counts) <- c("human1", "human2", "human3", "mock1", "mock2", "mock3", "mouse1", "mouse2", "mouse3", "soil1", "soil2", "soil3")


for(r in regions){
  shared <- read.table(file=paste(r, "/", r, ".trim.unique.good.filter.unique.precluster.pick.an.shared", sep=""), header=T)
  rownames(shared) <- shared$Group
  shared <- shared[,-c(1,2,3)]

  groups <- gsub(".v\\d*", "", rownames(shared))
  stopifnot(sum(groups == colnames(counts)) == length(groups))
  nseqs <- apply(shared, 1, sum)
  counts[r,] <- nseqs
}
```

Unfortunately, those counts are kind of depressing:

|   | human1  | human2	| human3	| mock1		| mock2		| mock3		| mouse1	| mouse2	| mouse3	| soil1		| soil2	| soil3	|
|-----------------------------------------------------------------------------------------------------------------------|
| v13 |  1038 |   810   | 1062   |  750     |  767    |  792    |  546    | 1049    |  365    |  790    |   606 |  870  |
| v15 |   556 |   700   | 1398   | 1459     | 1866    | 1669    | 1035    |  671    | 1345    | 1259    |  1480 | 1498  |
| v16 |   492 |   437   |  592   |  270     |  244    |  247    |  262    |  448    |  286    |  358    |   299 |  210  |
| v19 |   165 |    55   |  134   |  455     |  402    |  357    |  314    |  109    |  117    |  399    |   479 |  710  |
| v35 |  1050 |   965   |  848   |  979     | 1247    | 1231    |  634    |  606    |  425    |  612    |   357 | 1075  |
| v4  |   620 |   908   |  572   | 3721     | 4101    | 4857    | 1527    |  979    | 1186    |  728    |  2707 | 1310  |

Let's merge the three replicates for each sample like we did for the error analysis and move forward... First we'll make a mapping file and then we'll run merge.groups in mothur

```{r}
for(r in regions){
  write.table(cbind(paste(colnames(counts), ".", r, sep=""), gsub("\\d", "", colnames(counts))), file=paste(r, "/merge.groups", sep=""), quote=F, row.names=F, col.names=F, sep="\t")
}
```

```{r engine='bash'}
for REGION in v*
do
    mothur "#merge.groups(shared=$REGION/$REGION.trim.unique.good.filter.unique.precluster.pick.an.shared, design=$REGION/merge.groups)"
done
```

Here's what we get out when we run the above code again on the meged groups:


```{r}
counts <- matrix(rep(0, length(regions)*4), nrow=length(regions))
rownames(counts) <- regions
colnames(counts) <- c("human", "mock","mouse", "soil")

for(r in regions){
  shared <- read.table(file=paste(r, "/", r, ".trim.unique.good.filter.unique.precluster.pick.an.merge.shared", sep=""), header=T)
  rownames(shared) <- shared$Group
  shared <- shared[,-c(1,2,3)]

  groups <- gsub(".v\\d*", "", rownames(shared))
  stopifnot(sum(groups == colnames(counts)) == length(groups))
  nseqs <- apply(shared, 1, sum)
  counts[r,] <- nseqs
}
```

The merged counts:

|     | human | mock   | mouse | soil
|-------------------------------------
| v13 | 2910  |  2309  | 1960  | 2266
| v15 | 2654  |  4994  | 3051  | 4237
| v16 | 1521  |   761  |  996  |  867
| v19 |  354  |  1214  |  540  | 1588
| v35 | 2863  |  3457  | 1665  | 2044
| v4  | 2100  | 12679  | 3692  | 4745

Now we want to summarize the number of OTUs we observed under different conditions:

```{r}
getOTUCounts <- function(region){
  perfect <- read.table(file=paste0(region, "/HMP_MOCK.filter.pick.phylip.an.summary"), header=T)
  nochims <- read.table(file=paste0(region, "/", region, ".mock.precluster.pick.an.ave-std.summary"), header=T)
  observed <- read.table(file=paste0(region, "/", region, ".trim.unique.good.filter.unique.precluster.pick.an.merge.groups.ave-std.summary"), header=T)
  return(c(perfect[1L,2], nochims[1,3], observed[2,4], observed[4,4], observed[3,4], observed[1,4]))
}

otu.table <- t(sapply(regions, getOTUCounts))
colnames(otu.table) <- c("perfect", "no.chims", "obs.mock", "soil", "mouse", "human")

# v13      20   35.937   55.401 305.985 122.231 88.773
# v15      20   27.474   28.714 285.448  64.535 62.028
# v16      20   40.322   69.514 306.604 117.886 80.746
# v19      20   25.501   35.292 279.116  73.578 83.000
# v35      20   57.477   76.115 279.711 177.080 72.076
# v4       19   21.007   22.385 245.987  57.993 47.201
```





### Increasing sequence length improves classification

```{r}
countGoodBootstraps <- function(nameConf, cutoff=80){
  bootstrap <- gsub(".*\\((\\d*)\\).*", "\\1", nameConf)
  bootstrap[bootstrap == "unclassified"] = 0
  return(sum(as.numeric(bootstrap) >= cutoff))
}


getDepths <- function(taxFileName, cutoff=80){
  tax <- scan(taxFileName, what="", quiet =T)
  lines <- 1:length(tax)
  seqNames <- tax[lines %% 2 == 1]
  taxString <- tax[lines %% 2 == 0]
  taxList <- strsplit(taxString, ";")
  depths <- unlist(lapply(taxList, countGoodBootstraps))
  names(depths) <- seqNames
  return(depths)
}

for(r in regions){
  rdpFileName <- paste(r, "/", r, ".trim.unique.good.filter.unique.precluster.pick.pds_rdp.wang.taxonomy", sep="")
  rdp <- getDepths(rdpFileName)

  ggFileName <- paste(r, "/", r, ".trim.unique.good.filter.unique.precluster.pick.pds_gg.wang.taxonomy", sep="")
  gg <- getDepths(ggFileName)

  silvaFileName <- paste(r, "/", r, ".trim.unique.good.filter.unique.precluster.pick.nr_v119.wang.taxonomy", sep="")
  silva <- getDepths(silvaFileName)

  write.table(cbind("rdp" = rdp, "gg" = gg, "silva"=silva), file=paste(r, "/", r, ".tax.compare", sep=""), quote=F)


  rdpMock <- paste(r, "/", r, ".mock.precluster.pds_rdp.wang.taxonomy", sep="")
  rdp <- getDepths(rdpMock)

  ggMock <- paste(r, "/", r, ".mock.precluster.pds_gg.wang.taxonomy", sep="")
  gg <- getDepths(ggMock)

  silvaMock <- paste(r, "/", r, ".mock.precluster.nr_v119.wang.taxonomy", sep="")
  silva <- getDepths(silvaMock)

  write.table(cbind("rdp" = rdp, "gg" = gg, "silva"= silva), file=paste(r, "/mock.tax.compare", sep=""), quote=F)  
}
```

# Not sure about this...
# Now we'd like to break this down by sample type. First we'll count the number
# of times each sequence shows up in each group using mothur:

```{r engine='bash'}
for REGION in v*
do
    mothur "#merge.groups(group=$REGION/$REGION.good.pick.groups, design=$REGION/merge.groups);
            count.seqs(group=$REGION/$REGION.good.pick.merge.groups, name=$REGION/$REGION.trim.unique.good.filter.unique.precluster.pick.names)"
done
```

```{r}
getDepthByLibrary <- function(region){
  count.file <- paste(region, "/", region, ".trim.unique.good.filter.unique.precluster.pick.count_table", sep="")
  count.table <- read.table(file=count.file, header=T, row.names=1)

  depth.file <- paste(region, "/", region, ".tax.compare", sep="")
  depth.table <- read.table(file=depth.file, header=T, row.names=1)
  depth.table$rdp <- factor(depth.table$rdp, levels=0:7)
  depth.table$gg <- factor(depth.table$gg, levels=0:7)
  depth.table$silva <- factor(depth.table$silva, levels=0:7)

  mock.depth <- depth.table[count.table$mock > 0,]
  human.depth <- depth.table[count.table$human > 0,]
  mouse.depth <- depth.table[count.table$mouse > 0,]
  soil.depth <- depth.table[count.table$soil > 0,]

  mock.gg <- 100*summary(mock.depth$gg)/nrow(mock.depth)
  human.gg <- 100*summary(human.depth$gg)/nrow(human.depth)
  mouse.gg <- 100*summary(mouse.depth$gg)/nrow(mouse.depth)
  soil.gg <- 100*summary(soil.depth$gg)/nrow(soil.depth)

  mock.rdp <- 100*summary(mock.depth$rdp)/nrow(mock.depth)
  human.rdp <- 100*summary(human.depth$rdp)/nrow(human.depth)
  mouse.rdp <- 100*summary(mouse.depth$rdp)/nrow(mouse.depth)
  soil.rdp <- 100*summary(soil.depth$rdp)/nrow(soil.depth)

  mock.silva <- 100*summary(mock.depth$silva)/nrow(mock.depth)
  human.silva <- 100*summary(human.depth$silva)/nrow(human.depth)
  mouse.silva <- 100*summary(mouse.depth$silva)/nrow(mouse.depth)
  soil.silva <- 100*summary(soil.depth$silva)/nrow(soil.depth)

  return(rbind(mock.rdp, human.rdp, mouse.rdp, soil.rdp, mock.gg, human.gg, mouse.gg, soil.gg, mock.silva, human.silva, mouse.silva, soil.silva))
}

#composite <-array(0, dim=c(8,6,8))
composite <- data.frame(matrix(rep(0, 8*6*12), ncol=8))
colnames(composite) <- 0:7
composite[1:12,] <- getDepthByLibrary("v4");
composite[13:24,] <- getDepthByLibrary("v35")
composite[25:36,] <- getDepthByLibrary("v13")
composite[37:48,] <- getDepthByLibrary("v15")
composite[49:60,] <- getDepthByLibrary("v16")
composite[61:72,] <- getDepthByLibrary("v19")

composite$region <- c(rep("v4", 12), rep("v35", 12), rep("v13", 12), rep("v15", 12), rep("v16", 12), rep("v19", 12))
composite$database <- rep(c(rep("rdp", 4), rep("gg", 4), rep("silva", 4)), 6)
composite$sample <- rep(c("mock", "human", "mouse", "soil"), 18)
composite$total <- composite[,"6"] + composite[,"7"]
write.table(file="taxonomy.depth.analysis", composite, quote=F)
```

Let's build some dot-plots to show how well the various regions classified for each sample

```{r}
  data <- read.table(file="taxonomy.depth.analysis", header=T)
  rdp <- cbind("Mock"=data[data$database=="rdp" & data$sample=="mock", "total"], "Human"=data[data$database=="rdp" & data$sample=="human", "total"],
               "Mouse"=data[data$database=="rdp" & data$sample=="mouse", "total"], "Soil"=data[data$database=="rdp" & data$sample=="soil", "total"])
  rownames(rdp) <- c("V4", "V3-V5", "V1-V3", "V1-V5", "V1-V6", "V1-V9")

  gg <- cbind("Mock"=data[data$database=="gg" & data$sample=="mock", "total"], "Human"=data[data$database=="gg" & data$sample=="human", "total"],
               "Mouse"=data[data$database=="gg" & data$sample=="mouse", "total"], "Soil"=data[data$database=="gg" & data$sample=="soil", "total"])
  rownames(gg) <- c("V4", "V3-V5", "V1-V3", "V1-V5", "V1-V6", "V1-V9")

  silva <- cbind("Mock"=data[data$database=="silva" & data$sample=="mock", "total"], "Human"=data[data$database=="silva" & data$sample=="human", "total"],
               "Mouse"=data[data$database=="silva" & data$sample=="mouse", "total"], "Soil"=data[data$database=="silva" & data$sample=="soil", "total"])
  rownames(silva) <- c("V4", "V3-V5", "V1-V3", "V1-V5", "V1-V6", "V1-V9")

  gg.sp <- cbind("Mock"=data[data$database=="gg" & data$sample=="mock", "X7"], "Human"=data[data$database=="gg" & data$sample=="human", "X7"],
               "Mouse"=data[data$database=="gg" & data$sample=="mouse", "X7"], "Soil"=data[data$database=="gg" & data$sample=="soil", "X7"])
  rownames(gg.sp) <- c("V4", "V3-V5", "V1-V3", "V1-V5", "V1-V6", "V1-V9")


  pdf(file="classification.pdf", width=7, height=7)
  par(mar=c(5, 5, 0.5, 1))
  dotchart(gg, xlim=c(0,100), col="black", xlab="Unique reads that classified\nto genus or species level (%)", pch=19)
  points(x=rdp[,"Mock"], y=25:30, pch=15, col="black")
  points(x=rdp[,"Human"], y=17:22, pch=15, col="black")
  points(x=rdp[,"Mouse"], y=9:14, pch=15, col="black")
  points(x=rdp[,"Soil"], y=1:6, pch=15, col="black")

  points(x=silva[,"Mock"], y=25:30, pch=17, col="black")
  points(x=silva[,"Human"], y=17:22, pch=17, col="black")
  points(x=silva[,"Mouse"], y=9:14, pch=17, col="black")
  points(x=silva[,"Soil"], y=1:6, pch=17, col="black")


  points(x=gg.sp[,"Mock"], y=25:30, pch=21, bg="gray")
  points(x=gg.sp[,"Human"], y=17:22, pch=21, bg="gray")
  points(x=gg.sp[,"Mouse"], y=9:14, pch=21, bg="gray")
  points(x=gg.sp[,"Soil"], y=1:6, pch=21, bg="gray")

  legend(x=63, y=12, legend=c("RDP (gen.)", "SILVA (gen.)", "greengenes (gen.+sp.)", "greengenes (sp.)"), pch=c(15, 17, 19, 21), col="black", pt.bg=c("black", "black", "black", "gray"), bg="white")
  dev.off()
```

We classified all of the sequence data we generated using the naïve Bayesian
classifier using the RDP, SILVA, and greengenes reference taxonomies (Figure
Classification). In general, increasing the length of the region improved the
ability to assign the sequence to a genus or species. Interestingly, each of the
samples we analyzed varied in their ability to assign sequences to the depth of
genus or species and the reference database that did the best job of classifying
the sequences varied by sample type. For example, the SILVA reference did the
best for the human fecal samples and the RDP did the best for the mouse feces
and soil samples. An advantage of the greengenes database is that it contains
information for 2,514 species-level lineages for 11% of the reference sequences;
the other databases only provide taxonomic data to the genus level. Although
there was a modest association between the length of the fragment and the
ability to classify sequences to the species-level for the human samples, there
was no such association for the mouse and soil samples. In fact, at most 6.0% of
the soil sequences and 4.6% of the mouse sequences could be classified to a
genus. These results indicate that the ability to classify sequences to the
genus or species level is a function of read length, sample type, and the
reference database.  


### Sequencing errors are not random


# Not sure this is needed...
# It is commonly said that the error profile in PacBio-generated sequence data
# are random. As we saw above, the substitution preference was random. However,
# if the errors are truly random, then we would expect to have a bunch of
# singleton error sequences, not sequences with errors that show up multiple
# times. Let's see what type of error profile we have. Because they have the
# most reads, let's consider the v4, v35, and v15 mock community samples from
# before the pre.cluster step:
#
# ```{r, engine='bash'}
# for REGION in $(ls -d v*)
# do
#   mothur "#get.groups(fasta=$REGION/$REGION.trim.unique.good.filter.unique.fasta, name=$REGION/$REGION.trim.unique.good.filter.names, group=$REGION/$REGION.good.groups, groups=mock1.$REGION-mock2.$REGION-mock3.$REGION);
#           degap.seqs();
#           seq.error(fasta=current, name=current, reference=$REGION/HMP_MOCK.filter.fasta, aligned=F, processors=8)"
# done
# ```

... on into R ...

```{r}
error <- read.table(file="v4.trim.unique.good.filter.unique.pick.ng.error.summary", header=T, row.names=1)
error.nochim <- error[error$numparents==1,]
one.off <- error.nochim[error.nochim$mismatches==1,]
one.off.sum <- sum(one.off$weight)
v4.table <- table(one.off$weight)
v4.total <- sum(error$weight)



error <- read.table(file="v35.trim.unique.good.filter.unique.pick.ng.error.summary", header=T, row.names=1)
error.nochim <- error[error$numparents==1,]
one.off <- error.nochim[error.nochim$mismatches==1,]
v35.table <- table(one.off$weight)
v35.total <- sum(error$weight)


error <- read.table(file="v15.trim.unique.good.filter.unique.pick.ng.error.summary", header=T, row.names=1)
error.nochim <- error[error$numparents==1,]
one.off <- error.nochim[error.nochim$mismatches==1,]
v15.table <- table(one.off$weight)
v15.total <- sum(error$weight)
```

Let's plot the number of expected errors, assuming they're random, if the fragment is 1458 nt long...

```{r}
x <- 0:20
e1 <- 0.0049
e2 <- 0.0034
e3 <- 0.002
e4 <- 0.001

pdf(file="error.rate.pdf")
plot(x, dbinom(x, 1458, e1), type="l", ylim=c(0,0.35), xlab="Number of errors", ylab="% of full-length 16S rRNA gene sequences", yaxt="n", lwd=3)
points(x, dbinom(x, 1458, e2), type="l", col="red", lwd=3)
points(x, dbinom(x, 1458, e3), type="l", col="blue", lwd=3)
points(x, dbinom(x, 1458, e4), type="l", col="darkgreen", lwd=3)
axis(2, at=seq(0,0.35,0.05), label=seq(0,35,5), las=1)
legend(x=12, y=0.30, legend=c("0.49%", "0.34%", "0.20%","0.10%"), lwd=3, col=c("black", "red", "blue", "darkgreen"))
dev.off()
```


Although there was no obvious bias in the substitution, insertion, or deletion
patterns, during the pre-clustering analysis we noted that the frequency
distribution of sequences observed was not uniform. This indicated a lack of
randomness in the error profile. Because we were able to obtain a large number
of reads from the V1-V5 (N=5109 sequences), V4 (N=12684), and V3-V5 (N=3677)
datasets, we investigated the mock communities from these regions further. We
identified all of the sequences that had a 1-nt difference to the true sequence.
A majority of these sequences only appeared once; however, we observed sequences
that appeared 19 (V1-V5), 60 (V4), and 14 (V3-V5) times. Manual inspection of
these sequences indicated that they were not intra-genomic chimeras, but a
single base substitution, deletion, and insertion, respectively. The presence of
these errors at relative abundances between 0.37 and 0.47% indicates that
removing singleton sequences or OTUs will give a false sense of sequence
curation since many errors will be observed more than once. Furthermore,
recently described oligotyping methods would need to be reassessed since those
methods assume that every base in a sequence is equally likely to be incorrect.





## Conclusions
The various sequencing platforms that are available to microbial ecologists are
able to fill unique needs and have their own strengths and weaknesses. For
sequencing the 16S rRNA gene, the 454 platform is able to generate a moderate
number of high-quality 500-nt sequence fragments (error rates < 0.02%) [ref] and
the MiSeq platform is able to generate a large number of high-quality 250-nt
sequence fragments (error rates < 0.02%) [ref]. The promise of the PacBio
sequencing platform was the generation of high-quality near full-length sequence
fragments. As we have shown in this study, it is possible to generate near
full-length sequences; however, the error rate associated with those reads is
considerable (i.e. 0.34%) and requires a level of sequencing coverage that is
not commonly observed in a typical sequencing run. This results in the
generation of a small number of low quality full-length sequences. When we
considered the shorter V4 region, that is similar in length to what is sequenced
by the MiSeq platform, the error rates were nearly 5-fold higher than what has
previously been reported (0.09%). It appears that the promise offered by the
PacBio platform has not been realized.

The widespread adoption of the 454 and MiSeq platforms and decrease in Sanger
sequencing for sequencing the 16S rRNA gene has resulted in a decrease in the
generation of the full-length reference sequences that are needed for performing
phylogenetic analyses and designing lineage specific PCR primers and fluorescent
in situ hybridization probes. It remains to be determined whether the elevated
error rates we observed for full-length sequences are prohibitive for these
applications. Considering the 16S rRNA gene fragment is approximately 1,500 nt
long and an error rate of 0.34%, one would expect 26% of the sequences to have
no errors, 35% to have one error, 24% to have two errors, and 11% to have 3
errors.

Figure FullLengthErrors

Classification is dependent on quality of sequence data, length of the data, and
the quality of the database

Critical that people begin to utilize mock communities as part of their
experimental design so that they can quantify their error rates

Probably not worth the effort at this point


## Acknowledgements
The Genomic DNA from Microbial Mock Community A (Even, Low Concentration, v3.1,
HM-278D) was obtained through the NIH Biodefense and Emerging Infections
Research Resources Repository, NIAID, NIH as part of the Human Microbiome
Project.


## Funding statement
This study was supported by grants from the NIH (R01HG005975, R01GM099514 and
P30DK034933 to PDS and U54HG004973 to SKH).

References




## Figures

### Figure 1
```{r FigureErrorQualityScore, echo=F, eval=T, fig.width=6.5, fig.height=3.0}
layout(matrix(c(1,2,3), nrow=1), heights=1, widths=c(0.75,1,1))

#
#Panel A depicting range of quality scores for different errors
#
par(mar=c(6, 5, 0.5, 0.5))
plot(1, xlim=c(0.5, 4.5), ylim=c(0,75), type="n", yaxt="n", xaxt="n", xlab="", ylab="")
polygon(x=c(0.75, 1.25, 1.25, 0.75), y=c(m.quant["25%"], m.quant["25%"], m.quant["75%"], m.quant["75%"]))
polygon(x=c(1.75, 2.25, 2.25, 1.75), y=c(s.quant["25%"], s.quant["25%"], s.quant["75%"], s.quant["75%"]))
polygon(x=c(2.75, 3.25, 3.25, 2.75), y=c(i.quant["25%"], i.quant["25%"], i.quant["75%"], i.quant["75%"]))
polygon(x=c(3.75, 4.25, 4.25, 3.75), y=c(a.quant["25%"], a.quant["25%"], a.quant["75%"], a.quant["75%"]))
segments(x0=seq(0.75, 3.75, 1), x1=seq(1.25,4.25,1), y0=c(m.quant["50%"], s.quant["50%"], i.quant["50%"], a.quant["50%"]), lwd=4)

arrows(x0=1:4, y0=c(m.quant["75%"], s.quant["75%"], i.quant["75%"], a.quant["75%"]), y1=c(m.quant["97.5%"], s.quant["97.5%"], i.quant["97.5%"], a.quant["97.5%"]), angle=90, length=0.05)
arrows(x0=1:4, y0=c(m.quant["25%"], s.quant["25%"], i.quant["25%"], a.quant["25%"]), y1=c(m.quant["2.5%"], s.quant["2.5%"], i.quant["2.5%"], a.quant["2.5%"]), angle=90, length=0.05)

axis(2, las=2, at=c(0,25,50,75))
axis(1, at=c(1,2,3,4), label=c("Matches", "Substitutions", "Insertions", "Ambiguous"), las=2)
title(ylab="Quality score")
mtext(side=2, line=2.5, at=75, text="A", cex=2, font=2, las=2)



#
#Panel B depicting relationship between errors in barcodes/primers and the
#rest of the sequence.
#

par(mar=c(5,5,0.5,0.5))
plot(err.bcprimer$x[1:6,1]~err.bcprimer$Group.1[1:6], pch=19, ylab="Error rate (%)", xlab="Total mismatches to\nbarcodes and primers", axes=F, ylim=c(0,5))
axis(1)
axis(2, las=2)
box()
mtext(side=2, line=2.5, at=5, text="B", cex=2, font=2, las=2)



#
#Panel C depicting relationship between sequencing coverage and error rate for the different
#regions that were sequence
#


plotLines <- function(folder){
	composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
	composite <- composite[composite$reason=="g",]
	error.by.depth <- aggregate(composite$error, by=list(composite$freq), function(x){c(mean=mean(x), quantile(x, prob=c(0.025, 0.975)), n=length(x))})

	error.by.depth <- error.by.depth[error.by.depth$x[,"n"]>=50,] #exclude any coverage values where we don't have at least 50 observations
	fold.coverage <- error.by.depth[, "Group.1"]
	error.by.depth <- error.by.depth$x

	mean <- error.by.depth[,"mean"]
	points(mean~fold.coverage, type="l", lwd=2, col=clrs[folder])
}

par(mar=c(5,2,0.5, 0.5))
plot(1, xlim=c(2,30), ylim=c(0,0.05), type="n", xlab="", ylab="", yaxt="n")
title(xlab="Coverage", ylab="")
#axis(2, at=seq(0,0.05, 0.01), label=seq(0,5, 1), las=2)
lapply(regions, plotLines)
legend(x=18, y=0.05, legend=pretty.region[regions], lty=1, lwd=2, col=clrs[regions])
mtext(side=2, line=0.5, at=0.05, text="C", cex=2, font=2, las=2)

layout(1)
```



### Figure 2
```{r pipelineErrorFigure}
error.fraction <- cbind(initial.error=init.rates, basic.error=basic.error, bcprimer.error, coverage.error, aveq.error, oligosCoverage.error, oligosQScore.error, coverageQScore.error, allFilters.error)
rownames(error.fraction) <- gsub("v(\\d*)", "\\1", rownames(error.fraction))

error <- 100 * error.fraction[,grepl("error", colnames(error.fraction))]
fraction <- 100 * error.fraction[,grepl("frac", colnames(error.fraction))]



getJitter <- function(x){
	jitter <- 0.15
	runif(6, x-jitter, x+jitter)
}

l <- layout(matrix(c(1,2,3), nrow=3), heights=c(1,1,0.5))

par(mar=c(0.5, 7, 0.5, 0.5))
plot(1, type="n", xlim=c(0.75, 11.25), ylim=c(0,1.25), axes=F, xlab="", ylab="", cex.lab=1.5)
text(x=getJitter(1.0), y=error[,3], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(2.0), y=error[,4], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(3.0), y=error[,5], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(4.0), y=error[,6], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(5.0), y=error[,7], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(6.0), y=error[,8], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(7.0), y=error[,9], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(8.0), y=error[,10], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(9.0), y=error[,11], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(10.0), y=error[,12], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(11.0), y=error[,13], label=rownames(error.fraction), cex=1.5)
mtext(text="Error rate (%)", side=2, line=4, cex.lab=1.2)
box()
axis(2, las=2, at=seq(0,1.25, 0.25), label=format(seq(0, 1.25, 0.25),nsmall=2), cex.axis=1.5)

abline(v=seq(1.5,10.5, 1), col="grey")
mtext(side=2, line=4, at=1.22, text="A", cex=3, font=2, las=1)


par(mar=c(0.5, 7, 0.5, 0.5))
plot(1, type="n", xlim=c(0.75, 11.25), ylim=c(0,100), axes=F, ylab="% of Reads Remaining\nfrom Basic Criteria ", xlab="", cex.lab=1.5)
text(x=getJitter(1.0), y=fraction[,1], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(2.0), y=fraction[,2], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(3.0), y=fraction[,3], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(4.0), y=fraction[,4], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(5.0), y=fraction[,5], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(6.0), y=fraction[,6], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(7.0), y=fraction[,7], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(8.0), y=fraction[,8], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(9.0), y=fraction[,9], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(10.0), y=fraction[,10], label=rownames(error.fraction), cex=1.5)
text(x=getJitter(11.0), y=fraction[,11], label=rownames(error.fraction), cex=1.5)

box()
abline(v=seq(1.5,10.5, 1), col="grey")
mtext(side=2, line=4, at=96, text="B", cex=3, font=2, las=1)
axis(2, las=2, at=seq(0,100, 25), label=seq(0, 100, 25), cex.axis=1.5)


plot(1, type="n", xlim=c(0.75, 11.25), ylim=c(0,1), axes=F, ylab="", xlab="")

text(x=c(1:11)+0.2, y=rep(1, 11), label=c("No Mismatches", "\u22641 Mismatch", "Coverage", "Quality Score", "No Mismatches\n& Coverage", "\u22641 Mismatch\n& Coverage", "No Mismatches\n& Quality", "\u22641 Mismatch\n& Quality", "Coverage & Quality", "No Mismatches,\nCoverage & Quality", "\u22641 Mismatch,\nCoverage & Quality"), srt=90, pos=2, cex=1.25)




layout(1)

```




### Figure 3 
Quality score
```{r FigureMinimumSlidingWindow}
errorMinQualityScore <- function(folder){
	write(folder, "")

	composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
	composite <- composite[composite$reason == "g",]

	binned <- aggregate(composite$error, by=list(round(composite$minQ)), mean)
	binned <- binned[binned$Group.1 <= 70, ]
	points(binned$Group.1, binned$x, type="l", col=clrs[folder], lwd=2)
}

par(mar=c(5,5,0.5, 0.5))
plot(1, xlim=c(10,75), ylim=c(0,0.10), type="n", yaxt="n", xlab="", ylab="")
lapply(regions, errorMinQualityScore)
title(xlab="Minimum average quality score\nwithin a 50-nt window across the full sequence", ylab="Error Rate (%)")
axis(2, las=2, at=seq(0,0.1,0.02), label=seq(0,10,2))
legend(x=60, y=0.10, legend=pretty.region[regions], lty=1, lwd=2, col=clrs[regions])
```

```{r}

fractionMinQualityScore <- function(folder){
	write(folder, "")
	
	composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
	composite <- composite[composite$reason == "g",]
	
	binned <- aggregate(composite$error, by=list(round(composite$minQ)), length)	
	remaining <- 1-cumsum(binned$x)/sum(binned$x)
	points(binned$Group.1, remaining, type="l", col=clrs[folder], lwd=2)
}

par(mar=c(5,5,0.5, 0.5))
plot(1, xlim=c(10,75), ylim=c(0,1), type="n", yaxt="n", xlab="", ylab="")
lapply(regions, fractionMinQualityScore)
title(xlab="Minimum average quality score\nwithin a 50-nt window across the full sequence", ylab="Sequences above threshold (%)")
axis(2, las=2, at=seq(0,1,0.2), label=seq(0,100,20))
legend(x=10, y=0.30, legend=pretty.region[regions], lty=1, lwd=2, col=clrs[regions])
```

```{r}
aggregateErrorMinQualityScore <- function(folder){
write(folder, "")
	composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
	composite <- composite[composite$reason == "g",]
	

remaining.error <- rep(0, max(composite$minQ)+1)
qscores <- 0:max(composite$minQ)

remaining.error <- unlist(lapply(qscores, function(x){mean(composite[composite$minQ >= x,"error"])}))
names(remaining.error) <- qscores

points(x=10:70, remaining.error[10:70], type="l", col=clrs[folder], lwd=2)
}

par(mar=c(5,5,0.5, 0.5))
plot(1, xlim=c(10,75), ylim=c(0,0.02), type="n", yaxt="n", xlab="", ylab="")
lapply(regions, aggregateErrorMinQualityScore)
title(xlab="Minimum average quality score\nwithin 50-nt windows from across the full sequence", ylab="Error rate of sequences above threshold (%)")
axis(2, las=2, at=seq(0,0.02,0.005), label=seq(0,2,0.5))
legend(x=55, y=0.02, legend=pretty.region[regions], lty=1, lwd=2, col=clrs[regions])
```

```{r}
errorAveQualityScore <- function(folder){
write(folder, "")

	composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
	composite <- composite[composite$reason == "g",]
	
binned <- aggregate(composite$error, by=list(round(composite$aveQ)), mean)
binned <- binned[binned$Group.1 <= 70, ]
points(binned$Group.1, binned$x, type="l", col=clrs[folder], lwd=2)
}

par(mar=c(5,5,0.5, 0.5))
plot(1, xlim=c(10,75), ylim=c(0,0.10), type="n", yaxt="n", xlab="", ylab="")
lapply(regions, errorAveQualityScore)
title(xlab="Average quality score", ylab="Error Rate (%)")
axis(2, las=2, at=seq(0,0.1,0.02), label=seq(0,10,2))
legend(x=60, y=0.10, legend=pretty.region[regions], lty=1, lwd=2, col=clrs[regions])
```

```{r}
fractionAveQualityScore <- function(folder){
write(folder, "")

	composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
	composite <- composite[composite$reason == "g",]

binned <- aggregate(composite$error, by=list(round(composite$aveQ)), length)

remaining <- 1-cumsum(binned$x)/sum(binned$x)
points(binned$Group.1, remaining, type="l", col=clrs[folder], lwd=2)
}

par(mar=c(5,5,0.5, 0.5))
plot(1, xlim=c(10,75), ylim=c(0,1), type="n", yaxt="n", xlab="", ylab="")
lapply(regions, fractionAveQualityScore)
title(xlab="Average quality score", ylab="Sequences above threshold (%)")
axis(2, las=2, at=seq(0,1,0.2), label=seq(0,100,20))
legend(x=10, y=0.30, legend=pretty.region[regions], lty=1, lwd=2, col=clrs[regions])
```

```{r}
aggregateErrorAveQualityScore <- function(folder){
write(folder, "")
composite <- read.table(file=paste("pipeline_dev/", folder, "/", folder, ".composite", sep=""), header=T, row.names=1)
	composite <- composite[composite$reason == "g",]

remaining.error <- rep(0, max(composite$aveQ)+1)
qscores <- 0:max(composite$aveQ)

remaining.error <- unlist(lapply(qscores, function(x){mean(composite[composite$aveQ >= x,"error"])}))
names(remaining.error) <- qscores

points(x=10:70, remaining.error[10:70], type="l", col=clrs[folder], lwd=2)
}

par(mar=c(5,5,0.5, 0.5))
plot(1, xlim=c(10,75), ylim=c(0,0.02), type="n", yaxt="n", xlab="", ylab="")
lapply(regions, aggregateErrorAveQualityScore)
title(xlab="Average quality score\nacross the full sequence", ylab="Error rate of sequences above threshold (%)")
axis(2, las=2, at=seq(0,0.02,0.005), label=seq(0,2,0.5))
legend(x=55, y=0.02, legend=pretty.region[regions], lty=1, lwd=2, col=clrs[regions])
```


### Figure 4


## Supplemental Tables

SupplementalReasonsLost


```{r versions, echo=T, eval=T}
sessionInfo()
```
